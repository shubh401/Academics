{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import wordnet\n",
    "from tensorflow.contrib import rnn\n",
    "from html.parser import HTMLParser\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pretrained glove embeddings to embed words\n",
    "def get_embeddings_index():\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join('./glove.6B', 'glove.6B.300d.txt')) #TODO try 300 dimensions\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "embeddings_index = get_embeddings_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_words(word_list):\n",
    "    processed_word_list = []\n",
    "    for word in word_list.split():\n",
    "        if wordnet.synsets(word):\n",
    "            processed_word_list.append(lemmatizer.lemmatize(word))            \n",
    "    return processed_word_list     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHTMLParser(HTMLParser):  \n",
    "    a = ''\n",
    "    def handle_data(self, data):\n",
    "        self.a = self.a + str(data)        \n",
    "    def get_raw_text(self):\n",
    "        self.a = re.sub(r'[0-9_]+', ' ', self.a)\n",
    "        self.a = re.sub(r'[^\\w\\s]', ' ', self.a)        \n",
    "        return self.a    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing xml files to x train and y train data\n",
    "def preprocess_data(path = './en/'): \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for filename in os.listdir(path):    \n",
    "        root = ET.parse(path + filename).getroot()\n",
    "        #TODO add for other classifications, ie. age_group and multi-class \n",
    "        if(root.attrib['gender'] == 'male'):\n",
    "            y = 1\n",
    "        elif(root.attrib['gender'] == 'female'):\n",
    "            y = 0       \n",
    "\n",
    "        for text in root.findall('conversations/conversation'):            \n",
    "            parser = CustomHTMLParser()\n",
    "            parser.feed(str(text.text))\n",
    "            removed_tags = parser.get_raw_text()\n",
    "            word_list = preprocess_words(removed_tags)    \n",
    "            x_data.append(word_list)            \n",
    "            y_data.append(y)    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store pre-processed input\n",
    "def save_preprocessed_data(x_data, y_data): \n",
    "    with open(\"x_data_all_cnn.txt\", \"wb\") as f:   \n",
    "        pickle.dump(x_data, f)\n",
    "    with open(\"y_data_all_cnn.txt\", \"wb\") as f:   \n",
    "        pickle.dump(y_data, f)     \n",
    "save_preprocessed_data(x_data, y_data)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = \"\"\n",
    "y_data = \"\"\n",
    "with open(\"x_data_all_cnn.txt\", \"rb\") as f:   \n",
    "    x_data = pickle.load(f)\n",
    "with open(\"y_data_all_cnn.txt\", \"rb\") as f:   \n",
    "    y_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77188 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "def prepare_word_index(x_data):   \n",
    "    tokenizer = Tokenizer(num_words=50000) #max features is 50000\n",
    "    tokenizer.fit_on_texts(x_data)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index)) \n",
    "    return word_index\n",
    "word_index = prepare_word_index(x_data)\n",
    "\n",
    "def save_word_index(word_index):\n",
    "    with open(\"word_index.txt\", \"wb\") as f:   \n",
    "        pickle.dump(word_index, f)\n",
    "save_word_index(word_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_length = 500\n",
    "embedding_dim = 300 #change to other dim as well\n",
    "total = 163371\n",
    "def load_word_index():\n",
    "    with open(\"word_index.txt\", \"rb\") as f:   \n",
    "        word_index = pickle.load(f)\n",
    "        return word_index\n",
    "word_index = load_word_index()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_input(x_data, word_index, y_data, max_features=len(word_index)):\n",
    "    x = []\n",
    "    for text in x_data:\n",
    "        text_ids = []\n",
    "        for word in text:          \n",
    "            word_id = word_index.get(word, -1)\n",
    "            if word_id != -1 and word_id < max_features:\n",
    "                text_ids.append(word_id)\n",
    "        x.append(text_ids)\n",
    "\n",
    "    #pad sequence length to max_text_length\n",
    "    x = sequence.pad_sequences(x, maxlen=max_text_length, padding='post') \n",
    "    y = tf.keras.utils.to_categorical(y_data, num_classes=2)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load entire batch as training data\n",
    "def load_all_data(fx=\"x_data_all_cnn.txt\", fy=\"y_data_all_cnn.txt\", shuffle = False, seed=1000):\n",
    "    with open(fx, \"rb\") as f:   \n",
    "        x_data = pickle.load(f)\n",
    "    with open(fy, \"rb\") as f:   \n",
    "        y_data = pickle.load(f)    \n",
    "    x, y = get_network_input(x_data, load_word_index(), y_data, 10000)   \n",
    "    if(shuffle):\n",
    "        np.random.seed(seed)\n",
    "        r = np.arange(len(x))\n",
    "        np.random.shuffle(r)\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        x = x[r]\n",
    "        y = y[r]         \n",
    "    return x,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building embedding matrix\n",
    "def get_embedding_layer(embedding_dim):\n",
    "    word_index = load_word_index()\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    #embedding layer\n",
    "    embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                                input_length=max_text_length, trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_all_data(fx=\"x_data_all_cnn.txt\", fy=\"y_data_all_cnn.txt\", shuffle = True)\n",
    "#train on 140000 data samples out of 163371 samples, validation on 140000 to 150000\n",
    "train_num = 150000\n",
    "validation_num = 160000\n",
    "x_train = x[:train_num]\n",
    "y_train = y[:train_num]\n",
    "x_val = x[train_num:validation_num]\n",
    "y_val = y[train_num:validation_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          23156700  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 20, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 23,581,406\n",
      "Trainable params: 423,426\n",
      "Non-trainable params: 23,157,980\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def create_cnn_model():\n",
    "    #pure cnn model\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer(embedding_dim))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "    print(model.summary())\n",
    "    return model\n",
    "model12 = create_cnn_model()\n",
    "#model12.fit(x_train, y_train, epochs=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "#model12.save('model12_v1.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12.fit(x_train, y_train, epochs=50, initial_epoch=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "model12.save('./models/model12_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = load_model('./models/model12_v2.h5')\n",
    "model12.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = load_model('model12_v1.h5')\n",
    "model12.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12.fit(x_train, y_train, epochs=75, initial_epoch=50, batch_size=512, validation_data=(x_val, y_val))\n",
    "model12.save('./models/model12_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = load_model('./models/model12_v3.h5')\n",
    "model12.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_1():\n",
    "    #pure cnn model\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer(embedding_dim))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "    print(model.summary())\n",
    "    return model\n",
    "model13 = create_cnn_model_1()\n",
    "model13.fit(x_train, y_train, epochs=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "model13.save('model13_v1.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model13 = load_model('model13_v1.h5')\n",
    "model13.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model13.fit(x_train, y_train, epochs=50, initial_epoch=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "model13.save('./models/model13_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model13 = load_model('./models/model13_v2.h5')\n",
    "model13.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model_2():\n",
    "    #pure cnn model\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer(embedding_dim))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "    print(model.summary())\n",
    "    return model\n",
    "model14 = create_cnn_model()\n",
    "model14.fit(x_train, y_train, epochs=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "model14.save('model14_v1.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model14 = load_model('model14_v1.h5')\n",
    "model14.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model14.fit(x_train, y_train, epochs=50, initial_epoch=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "model14.save('./models/model14_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model14 = load_model('./models/model14_v2.h5')\n",
    "model14.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          23156700  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 20, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 23,581,406\n",
      "Trainable params: 423,426\n",
      "Non-trainable params: 23,157,980\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 150000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "150000/150000 [==============================] - 65s 437us/step - loss: 0.7307 - acc: 0.5340 - val_loss: 0.6905 - val_acc: 0.5076\n",
      "Epoch 2/25\n",
      "150000/150000 [==============================] - 61s 405us/step - loss: 0.6906 - acc: 0.5411 - val_loss: 0.6911 - val_acc: 0.5041\n",
      "Epoch 3/25\n",
      "150000/150000 [==============================] - 62s 412us/step - loss: 0.6869 - acc: 0.5458 - val_loss: 0.6904 - val_acc: 0.5062\n",
      "Epoch 4/25\n",
      "150000/150000 [==============================] - 60s 402us/step - loss: 0.6850 - acc: 0.5522 - val_loss: 0.7015 - val_acc: 0.5043\n",
      "Epoch 5/25\n",
      "150000/150000 [==============================] - 60s 403us/step - loss: 0.6839 - acc: 0.5538 - val_loss: 0.6972 - val_acc: 0.5143\n",
      "Epoch 6/25\n",
      "150000/150000 [==============================] - 61s 408us/step - loss: 0.6793 - acc: 0.5652 - val_loss: 0.6884 - val_acc: 0.5308\n",
      "Epoch 7/25\n",
      "150000/150000 [==============================] - 60s 402us/step - loss: 0.6739 - acc: 0.5733 - val_loss: 0.6830 - val_acc: 0.5398\n",
      "Epoch 8/25\n",
      "150000/150000 [==============================] - 61s 405us/step - loss: 0.6685 - acc: 0.5822 - val_loss: 0.6739 - val_acc: 0.5642\n",
      "Epoch 9/25\n",
      "150000/150000 [==============================] - 61s 406us/step - loss: 0.6612 - acc: 0.5935 - val_loss: 0.6644 - val_acc: 0.5855\n",
      "Epoch 10/25\n",
      "150000/150000 [==============================] - 61s 409us/step - loss: 0.6536 - acc: 0.6056 - val_loss: 0.6642 - val_acc: 0.5850\n",
      "Epoch 11/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.6443 - acc: 0.6179 - val_loss: 0.6741 - val_acc: 0.5658\n",
      "Epoch 12/25\n",
      "150000/150000 [==============================] - 69s 462us/step - loss: 0.6351 - acc: 0.6301 - val_loss: 0.6833 - val_acc: 0.5583\n",
      "Epoch 13/25\n",
      "150000/150000 [==============================] - 69s 463us/step - loss: 0.6227 - acc: 0.6445 - val_loss: 0.6636 - val_acc: 0.5852\n",
      "Epoch 14/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.6117 - acc: 0.6567 - val_loss: 0.6772 - val_acc: 0.5745\n",
      "Epoch 15/25\n",
      "150000/150000 [==============================] - 69s 463us/step - loss: 0.6007 - acc: 0.6683 - val_loss: 0.6914 - val_acc: 0.5625\n",
      "Epoch 16/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.5877 - acc: 0.6818 - val_loss: 0.7520 - val_acc: 0.5435\n",
      "Epoch 17/25\n",
      "150000/150000 [==============================] - 70s 463us/step - loss: 0.5777 - acc: 0.6906 - val_loss: 0.7458 - val_acc: 0.5508\n",
      "Epoch 18/25\n",
      "150000/150000 [==============================] - 71s 474us/step - loss: 0.5655 - acc: 0.7014 - val_loss: 0.6817 - val_acc: 0.5782\n",
      "Epoch 19/25\n",
      "150000/150000 [==============================] - 69s 463us/step - loss: 0.5528 - acc: 0.7121 - val_loss: 0.7650 - val_acc: 0.5457\n",
      "Epoch 20/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.5424 - acc: 0.7220 - val_loss: 0.7603 - val_acc: 0.5546\n",
      "Epoch 21/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.5307 - acc: 0.7319 - val_loss: 0.7117 - val_acc: 0.5758\n",
      "Epoch 22/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.5211 - acc: 0.7384 - val_loss: 0.7536 - val_acc: 0.5677\n",
      "Epoch 23/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.5119 - acc: 0.7458 - val_loss: 0.8245 - val_acc: 0.5468\n",
      "Epoch 24/25\n",
      "150000/150000 [==============================] - 70s 465us/step - loss: 0.5026 - acc: 0.7522 - val_loss: 0.8214 - val_acc: 0.5393\n",
      "Epoch 25/25\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4952 - acc: 0.7568 - val_loss: 0.8814 - val_acc: 0.5384\n"
     ]
    }
   ],
   "source": [
    "def create_cnn_model_3():\n",
    "    #pure cnn model\n",
    "    model = Sequential()\n",
    "    model.add(get_embedding_layer(embedding_dim))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(128, activation='relu'))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "    print(model.summary())\n",
    "    return model\n",
    "model15 = create_cnn_model_3()\n",
    "model15.fit(x_train, y_train, epochs=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "model15.save('model15_v1.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 332us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8814251465797425, 0.5384]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model15 = load_model('model15_v1.h5')\n",
    "model15.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 10000 samples\n",
      "Epoch 26/50\n",
      "150000/150000 [==============================] - 71s 470us/step - loss: 0.4859 - acc: 0.7625 - val_loss: 0.8118 - val_acc: 0.5595\n",
      "Epoch 27/50\n",
      "150000/150000 [==============================] - 70s 465us/step - loss: 0.4783 - acc: 0.7685 - val_loss: 0.8178 - val_acc: 0.5577\n",
      "Epoch 28/50\n",
      "150000/150000 [==============================] - 69s 463us/step - loss: 0.4723 - acc: 0.7730 - val_loss: 0.7473 - val_acc: 0.5765\n",
      "Epoch 29/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4658 - acc: 0.7770 - val_loss: 0.7420 - val_acc: 0.5778\n",
      "Epoch 30/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4589 - acc: 0.7811 - val_loss: 0.7585 - val_acc: 0.5729\n",
      "Epoch 31/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4529 - acc: 0.7851 - val_loss: 0.7690 - val_acc: 0.5705\n",
      "Epoch 32/50\n",
      "150000/150000 [==============================] - 69s 463us/step - loss: 0.4475 - acc: 0.7897 - val_loss: 0.8209 - val_acc: 0.5615\n",
      "Epoch 33/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4410 - acc: 0.7931 - val_loss: 0.8098 - val_acc: 0.5661\n",
      "Epoch 34/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4371 - acc: 0.7946 - val_loss: 0.8215 - val_acc: 0.5612\n",
      "Epoch 35/50\n",
      "150000/150000 [==============================] - 70s 465us/step - loss: 0.4315 - acc: 0.7984 - val_loss: 0.8140 - val_acc: 0.5684\n",
      "Epoch 36/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4252 - acc: 0.8020 - val_loss: 0.7991 - val_acc: 0.5659\n",
      "Epoch 37/50\n",
      "150000/150000 [==============================] - 70s 465us/step - loss: 0.4222 - acc: 0.8042 - val_loss: 0.7779 - val_acc: 0.5790\n",
      "Epoch 38/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4186 - acc: 0.8055 - val_loss: 0.8526 - val_acc: 0.5615\n",
      "Epoch 39/50\n",
      "150000/150000 [==============================] - 70s 465us/step - loss: 0.4118 - acc: 0.8098 - val_loss: 0.9073 - val_acc: 0.5507\n",
      "Epoch 40/50\n",
      "150000/150000 [==============================] - 70s 465us/step - loss: 0.4083 - acc: 0.8123 - val_loss: 0.8014 - val_acc: 0.5716\n",
      "Epoch 41/50\n",
      "150000/150000 [==============================] - 70s 464us/step - loss: 0.4053 - acc: 0.8143 - val_loss: 0.7950 - val_acc: 0.5773\n",
      "Epoch 42/50\n",
      "150000/150000 [==============================] - 71s 471us/step - loss: 0.4021 - acc: 0.8156 - val_loss: 0.8304 - val_acc: 0.5701\n",
      "Epoch 43/50\n",
      "150000/150000 [==============================] - 71s 476us/step - loss: 0.3992 - acc: 0.8188 - val_loss: 0.8884 - val_acc: 0.5517\n",
      "Epoch 44/50\n",
      "150000/150000 [==============================] - 66s 440us/step - loss: 0.3944 - acc: 0.8194 - val_loss: 0.9065 - val_acc: 0.5534\n",
      "Epoch 45/50\n",
      "150000/150000 [==============================] - 65s 431us/step - loss: 0.3918 - acc: 0.8221 - val_loss: 0.8205 - val_acc: 0.5740\n",
      "Epoch 46/50\n",
      "150000/150000 [==============================] - 65s 431us/step - loss: 0.3873 - acc: 0.8240 - val_loss: 0.9000 - val_acc: 0.5576\n",
      "Epoch 47/50\n",
      "150000/150000 [==============================] - 76s 507us/step - loss: 0.3872 - acc: 0.8244 - val_loss: 0.9203 - val_acc: 0.5500\n",
      "Epoch 48/50\n",
      "150000/150000 [==============================] - 96s 643us/step - loss: 0.3834 - acc: 0.8266 - val_loss: 0.8767 - val_acc: 0.5651\n",
      "Epoch 49/50\n",
      "150000/150000 [==============================] - 97s 644us/step - loss: 0.3801 - acc: 0.8272 - val_loss: 0.7863 - val_acc: 0.5871\n",
      "Epoch 50/50\n",
      "150000/150000 [==============================] - 97s 644us/step - loss: 0.3791 - acc: 0.8285 - val_loss: 0.8768 - val_acc: 0.5652\n"
     ]
    }
   ],
   "source": [
    "model15.fit(x_train, y_train, epochs=50, initial_epoch=25, batch_size=512, validation_data=(x_val, y_val))\n",
    "model15.save('./models/model15_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 334us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8767600864410401, 0.5652]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model15 = load_model('./models/model15_v2.h5')\n",
    "model15.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
